{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"MakeFunctions.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oi9e_1H-RK4R","executionInfo":{"status":"ok","timestamp":1622227742366,"user_tz":-120,"elapsed":19051,"user":{"displayName":"axel dinh van chi","photoUrl":"","userId":"10833900212226293025"}},"outputId":"41f401f2-9295-43e7-dfb1-30f116a4b58b"},"source":["from google.colab import drive\n","import os\n","drive.mount('/content/gdrive/')\n","import sys\n","sys.path.append('/content/gdrive/MyDrive/DeepLearning/Proj2')\n","os.chdir('gdrive/MyDrive/DeepLearning/Proj2')"],"id":"oi9e_1H-RK4R","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"djv05f4KL71Q","executionInfo":{"status":"ok","timestamp":1622228012831,"user_tz":-120,"elapsed":264371,"user":{"displayName":"axel dinh van chi","photoUrl":"","userId":"10833900212226293025"}},"outputId":"15b748f1-5fe7-4857-f9f0-84453cb07433"},"source":["%run -i gen_figures_results.py"],"id":"djv05f4KL71Q","execution_count":2,"outputs":[{"output_type":"stream","text":["========================================\n","Part 0: Training a MLP using ReLU as activation, MSE loss and the SGD optimizer:\n","\n","Training...\n","Training [1/1]: Best train error rate = 3.00%, Best test error rate = 2.90%\n","Training over...\n","========================================\n","Part 1: Training a MLP with different activations and weight initializations:\n","        We use the MSE loss and the SGD optimizer...\n","\n","Loading the results...\n","ReLU with default initialization:\n","\t\tMean Best Train Error = 2.36%(±0.61)\n","\t\tMean Best Test Error = 2.57%(±0.88)\n","ReLU with xavier initialization:\n","\t\tMean Best Train Error = 2.65%(±0.68)\n","\t\tMean Best Test Error = 2.72%(±0.74)\n","ReLU with he initialization:\n","\t\tMean Best Train Error = 2.36%(±0.60)\n","\t\tMean Best Test Error = 2.36%(±0.86)\n","Tanh with default initialization:\n","\t\tMean Best Train Error = 2.97%(±0.73)\n","\t\tMean Best Test Error = 2.95%(±1.49)\n","Tanh with xavier initialization:\n","\t\tMean Best Train Error = 3.29%(±0.82)\n","\t\tMean Best Test Error = 3.47%(±1.18)\n","Tanh with he initialization:\n","\t\tMean Best Train Error = 3.51%(±0.64)\n","\t\tMean Best Test Error = 3.46%(±1.66)\n","\n","It seems like using ReLU gives better results quite consistently\n","========================================\n","Part 2: Training a MLP with different losses and optimizers:\n","        We use ReLU activation and the 'default' initialization...\n","\n","Loading the results...\n","MSE loss with SGD optimizer:\n","\t\tMean Best Train Error = 2.70%(±0.41)\n","\t\tMean Best Test Error = 3.19%(±1.36)\n","MSE loss with Adam optimizer:\n","\t\tMean Best Train Error = 1.35%(±0.43)\n","\t\tMean Best Test Error = 2.03%(±0.64)\n","CrossEntropy loss with SGD optimizer:\n","\t\tMean Best Train Error = 2.15%(±0.69)\n","\t\tMean Best Test Error = 2.59%(±0.90)\n","CrossEntropy loss with Adam optimizer:\n","\t\tMean Best Train Error = 1.15%(±0.59)\n","\t\tMean Best Test Error = 2.15%(±0.81)\n","\n","It seems like using Adam gives better results quite consistently\n","========================================\n","Part 3: Training a MLP with a scheduler, the Cross Entropy loss, the Adam optimizer and 'default' weight initialization:\n","\n","training...\n","Training [1/1]: Best train error rate = 0.50%, Best test error rate = 1.80%\n","Training over...\n","\n","Training of the same model without scheduler:\n","Training...\n","Training [1/1]: Best train error rate = 1.50%, Best test error rate = 2.10%\n","Training over...\n","\n","Using a scheduler allows us to obtain even better results\n","========================================\n","Part 4: Training a MLP with Dropout (p = 0.1):\n","        We use a scheduler, the Cross Entropy loss, the Adam optimizer and 'default' weight initialization...\n","\n","training...\n","Training [1/1]: Best train error rate = 3.00%, Best test error rate = 2.20%\n","Training over...\n","\n","The train error here might be quite large, but not the test error.\n","This is because we evaluate the train error during the training, so the dropout is ON during the computation of the train error,\n"," thus inputs are randomly zeroed between layers, which leads to poor evaluation.\n","But for the test error, the dropout is OFF, i.e we get a real evaluation.\n","In any case, we get overall poor performance as the model is small and there is nearly no noise in the dataset.\n","(Hence the model can not overfit).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GX0wMwr3T2Dj","executionInfo":{"status":"ok","timestamp":1622228500883,"user_tz":-120,"elapsed":254854,"user":{"displayName":"axel dinh van chi","photoUrl":"","userId":"10833900212226293025"}},"outputId":"98702f3e-2a0c-4bdc-ddd2-45bca858b651"},"source":["%run -i test.py"],"id":"GX0wMwr3T2Dj","execution_count":3,"outputs":[{"output_type":"stream","text":["========================================\n","Part 0: Training a MLP using ReLU as activation, MSE loss and the SGD optimizer:\n","\n","Training...\n","Training [1/1]: Best train error rate = 2.50%, Best test error rate = 2.20%\n","Training over...\n","========================================\n","Part 1: Training a MLP with different activations and weight initializations:\n","        We use the MSE loss and the SGD optimizer...\n","\n","        For this part, we will load and display results made on 10 rounds to relax the computations:\n","Loading the results...\n","ReLU with default initialization:\n","\t\tMean Best Train Error = 2.36%(±0.61)\n","\t\tMean Best Test Error = 2.57%(±0.88)\n","ReLU with xavier initialization:\n","\t\tMean Best Train Error = 2.65%(±0.68)\n","\t\tMean Best Test Error = 2.72%(±0.74)\n","ReLU with he initialization:\n","\t\tMean Best Train Error = 2.36%(±0.60)\n","\t\tMean Best Test Error = 2.36%(±0.86)\n","Tanh with default initialization:\n","\t\tMean Best Train Error = 2.97%(±0.73)\n","\t\tMean Best Test Error = 2.95%(±1.49)\n","Tanh with xavier initialization:\n","\t\tMean Best Train Error = 3.29%(±0.82)\n","\t\tMean Best Test Error = 3.47%(±1.18)\n","Tanh with he initialization:\n","\t\tMean Best Train Error = 3.51%(±0.64)\n","\t\tMean Best Test Error = 3.46%(±1.66)\n","========================================\n","Part 2: Training a MLP with different losses and optimizers:\n","        We use ReLU activation and the 'default' initialization...\n","\n","        For this part, we will load and display results made on 10 rounds to relax the computations:\n","Loading the results...\n","MSE loss with SGD optimizer:\n","\t\tMean Best Train Error = 2.70%(±0.41)\n","\t\tMean Best Test Error = 3.19%(±1.36)\n","MSE loss with Adam optimizer:\n","\t\tMean Best Train Error = 1.35%(±0.43)\n","\t\tMean Best Test Error = 2.03%(±0.64)\n","CrossEntropy loss with SGD optimizer:\n","\t\tMean Best Train Error = 2.15%(±0.69)\n","\t\tMean Best Test Error = 2.59%(±0.90)\n","CrossEntropy loss with Adam optimizer:\n","\t\tMean Best Train Error = 1.15%(±0.59)\n","\t\tMean Best Test Error = 2.15%(±0.81)\n","========================================\n","Part 3: Training a MLP with a scheduler, the Cross Entropy loss, the Adam optimizer and 'default' weight initialization:\n","\n","training...\n","Training [1/1]: Best train error rate = 0.50%, Best test error rate = 1.40%\n","Training over...\n","\n","Training of the same model without scheduler:\n","Training...\n","Training [1/1]: Best train error rate = 0.20%, Best test error rate = 1.20%\n","Training over...\n","========================================\n","Part 4: Training a MLP with Dropout (p = 0.1):\n","        We use a scheduler, the Cross Entropy loss, the Adam optimizer and 'default' weight initialization...\n","\n","training...\n","Training [1/1]: Best train error rate = 3.00%, Best test error rate = 1.60%\n","Training over...\n"],"name":"stdout"}]}]}